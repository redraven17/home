---
title: Evolution- The Scalable Revolution
date: 2020-02-19
permalink: /posts/2012/08/blog-post-11/
author_profile: false
tags:
  - Deep Learning
  - Reinforcement Learning
---

Evolution Strategies (ES) have proven to be an effective technique for training continuous as well as discrete control tasks. By making use of Gaussian perterubations in the weight space, ES eliminate the need for backpropagation and reduce the computation time by a significant extent when making use of parallelization. This has allowed scalability in the Reinforcement Learning paradigm.  


<p align="center"><img src="/images/BipedalWalker.gif" align="center" height="200" width="200" /><img src="/images/LunarLander.gif" align="center" height="200" width="200" /><img src="/images/Hopper.gif" align="center" height="200" width="200" /><img src="/images/Swimmer.gif" align="center" height="200" width="200" /></p>  

<p align="center"><img src="/images/HalfCheetah.gif" align="center" height="200" width="200" /><img src="/images/InvertedPendulum.gif" align="center" height="200" width="200" /><img src="/images/Reacher.gif" align="center" height="200" width="200" /><img src="/images/Humanoid.gif" align="center" height="200" width="200" /></p>  

<p align="center"><img src="/images/Thrower.gif" align="center" height="200" width="200" /><img src="/images/HumanoidStandup.gif" align="center" height="200" width="200" /></p>  


This post is a naive implementation of ES. ES was proposed in OpenAI's blog post<sup>[1](https://openai.com/blog/evolution-strategies/)</sup> and paper<sup>[2](https://arxiv.org/pdf/1703.03864.pdf)</sup>. A detailed implementation of OpenAI's version can be found at their Github repository<sup>[3](https://github.com/openai/evolution-strategies-starter)</sup>.  


__Importing Dependencies__
====== 

We make use of OpenAI's gym environments, namely the classic control and MuJoCo suite. Our model implementation is carried out in PyTorch.  

```python
from google.colab import drive
user_name = '/content/drive'
drive.mount(user_name, force_remount=True)
import os
os.environ['OMP_NUM_THREADS'] = '1'
import numpy as np
import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks/')
import CyclicMDP
from CyclicMDP import CyclicMDP
import time
import datetime
!pip install tensorboardX
!pip install --upgrade multiprocessing
!pip3 install box2d-py==2.3.8
!pip install gym==0.15.4 
import tensorboardX
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.multiprocessing as mp
from torch import optim
import pickle as pkl

import scipy.stats as ss
from tensorboardX import SummaryWriter
import gym
checkpoint_name = '/content/drive/My Drive/Colab Notebooks/Checkpoint/'
os.sched_setaffinity(os.getpid(), {0})
os.system("taskset -p 0xffffffff %d" % os.getpid())
```  

__Network Architecture__
======  

We define a simple architecture consisting of Linear layers and *tanh* activation. *tanh* is a suitable choice here as all our action outputs being either torque values or physical force units in gym remain in the [-1,1] range.  

```python
class NeuralNetwork(nn.Module):
    '''
    Neural network for continuous action space
    '''
    def __init__(self, input_shape, n_actions):
        super(NeuralNetwork, self).__init__()

        self.mlp = nn.Sequential(
            nn.Linear(input_shape, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh())

        self.mean_l = nn.Linear(64, n_actions)
        self.mean_l.weight.data.mul_(0.1)

        self.var_l = nn.Linear(64, n_actions)
        self.var_l.weight.data.mul_(0.1)

        self.logstd = nn.Parameter(torch.zeros(n_actions))

    def forward(self, x):
        ot_n = self.mlp(x.float())
        return torch.tanh(self.mean_l(ot_n))
```  

__Algorithm Implementation__
======  

ES is genetic algorithm by origin and consists of multiple models. Each model is a part of a braoder set of models called the population. Models in a population are the offsprings from the parent model. Say we have a model (known as the parent model) and we perturb its weights by random Gaussian noise 100 times, we obtain a set of 100 new weights. Thus, we have 100 new models which can be optimized in the weight space. Upon running each of these models once we obtain the episodic reward and gauge their ability to learn. We then create a new model by taking a weighted average over the set of weights of the population. Models with higher returns are given higher preference over weak learners. In various implementations of Evolutionary Computing, only the top $x\%$ models are selected. This is called genetic selection. However, for the purpose of the tutorial we consider all the models in the population.  

The update rule for ES algorithm is mathematically expressed as-  

<p align='center'><img src='https://latex.codecogs.com/gif.latex?%5Ctheta_%7Bt&plus;1%7D%5Cleftarrow%20%5Ctheta_%7Bt%7D%20&plus;%20%5Calpha%5Cfrac%7B1%7D%7Bn%5Csigma%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DF_%7Bi%7D%5Cepsilon_%7Bi%7D' /></p>  

Here, *theta* are the parameters at time-step *t*, *alpha* the learning rate, *n* the number of models in population or simply the population size, *sigma* the mutation parameter *F* the reward corresponding to each model in the population and *epsilon* the Gaussian noise.  

```python
def sample_noise(neural_net):
    '''
    Sample noise for each parameter of the neural net
    '''
    nn_noise = []
    for n in neural_net.parameters():
        noise = np.random.normal(size=n.data.numpy().shape)
        nn_noise.append(noise)
    return np.array(nn_noise)

def evaluate_neuralnet(nn, env):
    '''
    Evaluate an agent running it in the environment and computing the total reward
    '''
    obs = env.reset()
    game_reward = 0

    while True:
        # Output of the neural net
        net_output = nn(torch.tensor(obs))
        # the action is the value clipped returned by the nn
        action = net_output.data.numpy().argmax()
        # action = np.clip(net_output.data.cpu().numpy().squeeze(), -1, 1)
        new_obs, reward, done, _ = env.step(action)
        obs = new_obs

        game_reward += reward

        if done:
            break

    return game_reward

def evaluate_noisy_net(noise, neural_net, env):
    '''
    Evaluate a noisy agent by adding the noise to the plain agent
    '''
    old_dict = neural_net.state_dict()

    # add the noise to each parameter of the NN
    for n, p in zip(noise, neural_net.parameters()):
        p.data += torch.FloatTensor(n * STD_NOISE)

    # evaluate the agent with the noise
    reward = evaluate_neuralnet(neural_net, env)
    # load the previous paramater (the ones without the noise)
    neural_net.load_state_dict(old_dict)

    return reward

def worker(params_queue, output_queue):
    '''
    Function execute by each worker: get the agent' NN, sample noise and evaluate the agent adding the noise. Then return the seed and the rewards to the central unit
    '''

    # env = gym.make(ENV_NAME)
    env = CyclicMDP()
    # actor = NeuralNetwork(env.observation_space.shape[0], env.action_space.n)
    actor = NeuralNetwork(3,3)
    while True:
        # get the new actor's params
        act_params = params_queue.get()
        if act_params != None:
            # load the actor params
            actor.load_state_dict(act_params)

            # get a random seed
            seed = np.random.randint(1e6)
            # set the new seed
            np.random.seed(seed)

            noise = sample_noise(actor)

            pos_rew = evaluate_noisy_net(noise, actor, env)
            # Mirrored sampling
            neg_rew = evaluate_noisy_net(-noise, actor, env)
            time.sleep(1)

            output_queue.put([[pos_rew, neg_rew], seed])
        else:
            break


def normalized_rank(rewards):
    '''
    Rank the rewards and normalize them.
    '''
    ranked = ss.rankdata(rewards)
    norm = (ranked - 1) / (len(ranked) - 1)
    norm -= 0.5
    return norm
```  

__Training Loop__
======  

We execute the typical Reinforcement Learning training loop for a finite number of episodes. The parameter *MAX\_WORKERS* should be handled carefully since training on multiple CPUs requires more processes and can often lead to poor performance. *sigma* is a tricky hyperparameter to tune. Often at times it will yield good evolutions in weight spaces but for some environments it may not lead to full convergence.  

__Scalability is the Key__
======  

As always, it is good practice to view the performance of our algorithm. In the case of ES we visualize the episodic and average rewards. We also gain intuition of the overall performance of our population by taking into the number of elite learners, which are defined as the number of models above the average score. As the population converges to a global minima, the number of elite learners increase to a constant indicating the successful convergence of the overall population.  

We discussed the CyclicMDP environment in the previous post<sup>[4](https://karush17.github.io/posts/2012/08/blog-post-10/)</sup>. The environment serves as a good minimal working example for algorithm and presents certain key elements of policy search which Actor-Critic algorithms often lack.  

<p align="center"><img src="/images/CyclicMDP-results.PNG" height="300" width="1200"></p>  

CyclicMDP results have mixed performance for various algorithms. In the case of PPO<sup>[5](https://arxiv.org/pdf/1707.06347.pdf)</sup>, the agent gets stuck on a local optima early in training as a result of proximal policy search. This can be interpreted as a difficult problem for the agent since it is only searching specific regions of the weight space as it is contrained to the trust region. On the other hand, Evolution Strategies performs significantly well. Primary reason behind this finding is that ES does not make use of backpropagation updates and uses multiple models as its agents to traverse through the weight space. The probability of ES finding the long-sighted global convergence is much higher in comparison to Policy Gradient methods.  

In terms of scalability, ES is parallelizable as it has small runtime per episode. Complex algorithms such as ES making use of multiple models in its population can be easily scaled up using parallel compute.  

Another notable finding is that CyclicMDP presents a good benchmark for hyperparameter analysis. Sigma being a tricky parameter for ES can be tuned with ease given domain-specific knowledge of the algorithm.  

__Limitations__
====== 

Although ES is significantly parallelizable and performs comparatively similar to other policy-based approaches, it does lack the following features-  
1. ES is less sample efficient when compared to modern-day RL approaches as a result of its random-walk behavior.  
2. Unlike Actor-Critic methods, ES may often diverge from its convergence as a result of continued perturbation in the weight space. This is often tackled by using a learning schedule for the learning rate.  
3. ES is sensitive to hyperparameters and requires tuning upto a significant extent.  

__References__
====== 
1. [OpenAI Blog](https://openai.com/blog/evolution-strategies/)  
2. [Paper](https://arxiv.org/pdf/1703.03864.pdf)  
3. [Github Repository](https://github.com/openai/evolution-strategies-starter)  
4.  [CyclicMDP Post](https://karush17.github.io/posts/2012/08/blog-post-10/)
5. [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)  





