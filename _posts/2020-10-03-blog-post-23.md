---
title: An Introduction to Ising Models as Computational Frameworks
date: 2020-10-03
permalink: /posts/2012/08/blog-post-23/
author_profile: false
tags:
  - Reinforcement Learning
  - Deep Learning
---


For long, Deep Learning has been motivated by the design and implementation of physics-based methods such as Boltzmann Machines and energy-based distributions for probabilistic estimations. This blog post takes the idea of physical implementations in deep learning a step further and introduces Ising models. Ising models have been the fundamental aspect of computations and approximations in the machine learning community. In fact, the famous neural network being used for majority of machine learning applications is itself a specific case of an Ising model carrying out probabilistic inference using a boltzmann distribution (also known as the softmax activation in the machine learning community). We will look at the need for adopting Ising models in the machine learning community and their capabilities from a computational perspective. Additionally, the later sections of the post will dive deeper into the working and operation of the model along with its various components. 

__Why the Ising Model?__
======  
The early 1920s saw physicists working on lattice structures which were intrinsically difficult to solve. These structures consisted of inherent computational properties and their dynamic behavior in different phases opened multiple questions about their physical and chemical characteristics. Naturally, solving these lattices became of interest to physicists who originally worked on lower dimensional models. Ernst Ising (below), a german physicist, solved the one-dimensional Ising Model in 1925 which was given to him as a problem by his advisor Wilhelm Lenz (below). Ising solved the model and indicated that no phase transitions occured between particles which would later be corrected for higher dimensions. Although Ising solved the one-dimensional model, the two dimensional square-lattice model is much harder to solve and was later looked at analytically by Lars Onsager in 1944. The two-dimensional can now be solved using transfer matrix method which is one of the many techniques used for solved lattices alongside quantum theory.  

<p align="center"><img src="/images/ising.png" height="200" width="200" />   <img src="/images/lenz.jfif" height="200" width="200" /></p>  
<p align="center"><em>Ernst Ising (1924), Wilhelm Lenz (1920)</em></p>  

One of the reasons why Ising model is famous for its lattice structure is that it lays out the interaction between its particles in an energy-based manner. Particles acquire spin values which allows them to interact with adjacent particles in the model, hence impacting the overall configuration of teh system. Eventually, particle-particle (or rather spin-spin) interactions end up minimiznig the energy of the system which results in a thermal equilibrium. Additionally, the model temperature can be annealed in simlutaion to excite the particles in a more drastic manner and assess their behavior which would eventually result in thermal equilibrium. Thermal equilibrium is a desirable from a computational perspective as it guarantees convergence in an algorithm. For instance, consider an algorithm which was constructed to solve the Ising model and does not have any knowledge of its particle positions. The algorithm will begin by tuning the temperature and particle spins in order to make the model converge. Once the model is converged, the problem is considered to be solved, indicating that the problem has atleast one solution. The equilibrium property of Ising model assures modern-day programmers that the problem can be solved and efficient solutions to approximations may exist.  

However, existence of a solution does not serve as a valid motviation for the usage of Ising model since there are multiple frameworks which guarantee the existence of many solutions in algorithms. The primary reason for the usage of Ising model is its structure. Ising model constitutes of particles arranged in a random configuration which rearrange themselves to yield optimal energy. Thi can be directly translated to modern-day neural networks which consist of particles (neurons) that adjust their weights (shift their positions in the lattice) to yield an optimal solution (energy). The deep structural connection between Ising lattices and computational graphs served as one of the major motivations for their adoption into the machine learning community back in the early 1970s. A number of different inference frameworks such as the Boltzmann Machine (BM), Restricted Boltzmann Machine (RBM) and Hopfield Network (HN) utilized energy-based objectives which were motivated by Ising computations. Additionally, the HN served as one of foundational examples of memory-based learning leveraging the interaction between neurons which is analogous to determining optimal spin-spin configurations in the lattice.  

<p align="center"><img src="/images/struct.jfif" height="200" width="200" />   <img src="/images/rbm.png" height="200" width="300" />   <img src="/images/hn.png" height="200" width="200" /></p>  
<p align="center"><em>Structural similarity between 2D Ising Model (left), RBM (center) and HN (right) highlighting the widespread adoption of the lattice framework</em></p>  

__But what exactly is an Ising Model?__
======  
Although there have been many definitions of the Ising model, we will look at the two most famous ones. We willf first define the ising model from a physical perspective as Ising did and then gain intuition to understand its definition from a computational perspective.  

The Ising Model is an energy-based model which consists of physical particles in a lattice structure. Particles acquire spin states in the lattice which affect the states of neighbouring particles as well as the overall configuration of the system. Consider an Ising model consisting of $N$ particles with $N \geq 1$ and $N \neq \infty$. The set of all particles can then be represented as $\{1,2,...,N\}$. Mathematically, let $\sigma_{i}$ be the spin state of particle $i$ in the system, then $\sigma_{i}$ may take a value from the set $\mathcal{S}$ which comprises of all the possible spin states. Most Ising models are bivariate which consist of only two spin states $\mathcal{S} = \{-1,+1\}$. In physics, the two spin states correspond to clockwise and counter-clockwise spins of particles of a ferromagnetic material.

<p align="center"><img src="/images/model.gif" height="100" width="300" /></p>  
<p align="center"><em>Bivariate spin configurations in a 1D Iising model</em></p>  

However, modern Ising models are often presented to contain more than two spin states. Particles in an Ising model interact with each other by means of their spin states. Consider two particles $i$ and $j$ with spin states $\sigma_{i}$ and $\sigma_{j}$. The interaction between the two particles $<\sigma_{i},\sigma_{j}> = \mu_{ij}$ can be mathematically expressed as $\mu_{ij} = \sigma_{i}.\sigma_{j}$. These interactions are also called spin-spin interactions since they take place between spin states of the two particles.

Computationally, the spin-spin interactions between any two particles can be regarded as a connection between two nodes of a computation graph. Consider two nodes A and B. These nodes interact with each other by means of a weight which determines the extent of information to be passed from node A to node B. More intuitively, the weight can be thought of as a controlling mechanism for governing the flow of input. In the case of Ising model, the weight can be thought of as an interaction between two particles which governs the overall change in the system. The value of the weight plays an essential role for the system to achieve thermal equilibrium. Thus, much of the properties of Ising models are leveraged using their particles and spin values from a computational perspective.  

