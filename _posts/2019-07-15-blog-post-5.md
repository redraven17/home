---
title: Learning Stock Prices using RNN and LSTM in PyTorch
date: 2019-07-15
permalink: /posts/2012/08/blog-post-15/
author_profile: false
tags:
  - Deep Learning
---

This notebook will introduce you to the basics of sequential learning and how it can be implemented in PyTorch. We will use Recurrent Neural Networks (RNNs) and Long-Short Term Memory Networks (LSTMs) for training. So let's get started!  

__Importing Dependencies__
====== 

We will make use of PyTorch as our Deep Learning module along with Pandas as our data management module. But before we move on with model training, let's see how our data looks.  

```python
#Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Import Dependencies
import numpy as np
import csv
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import pandas as pd

#Set GPU as Accelerator
if torch.cuda.is_available():
  device = torch.device('cuda')


path = r'/content/drive/My Drive/Colab Notebooks/prices-split-adjusted.csv'
df = pd.read_csv(path,index_col=0)
df = df[df.symbol=='EQIX']
df = df.drop(['symbol','volume'],axis=1)
labels = pd.DataFrame(df['high'])
data = df.drop('high',axis=1)
data.head()
```

__Analyze Data__
====== 

Okay, so now that we have a good understanding about the columns and samples of our dataset, let's preprocess it in order to make it model-friendly.  

First, we will split it into two sets, training set and test set.  

Now, we will reshape into the correct input format. Remember that sequential data must be a 3-dimensional array of the format $[batch\_size, seq\_len, features]$ where batch_size is the number of samples in each batch, seq_len is the length of each sequence and features are the various different sequences being fed into the model.  

```python
#Read Data
def read_data():
  path = r'/content/drive/My Drive/Colab Notebooks/prices-split-adjusted.csv'
  df = pd.read_csv(path,index_col=0)
  df.info()
  df.head()
  print('Number of different stocks: ',len(list(set(df.symbol))))
  return df

#Visualiza a particular stock
def visualize_stock():
  plt.figure()
  plt.plot(df[df.symbol=='EQIX'].open.values, color='red',label='open')
  plt.plot(df[df.symbol=='EQIX'].close.values, color='green',label='close')
  plt.plot(df[df.symbol=='EQIX'].low.values, color='blue',label='low')
  plt.plot(df[df.symbol=='EQIX'].high.values, color='black',label='high')
  plt.title('Stock Price (EQIX)')
  plt.xlabel('Time (Days)')
  plt.ylabel('Price (USD)')
  plt.legend(loc='best')
  plt.show()

#Normalize Price values
def normalize(df, stock_name):
  df_stock = df[df.symbol == stock_name].copy()
  df_stock.drop(['symbol'],1,inplace=True)
  df_stock.drop(['volume'],1,inplace=True)
  min_max_scaler = sklearn.preprocessing.MinMaxScaler()
  df_stock['open'] = min_max_scaler.fit_transform(df_stock.open.values.reshape(-1,1))
  df_stock['close'] = min_max_scaler.fit_transform(df_stock.close.values.reshape(-1,1))
  df_stock['high'] = min_max_scaler.fit_transform(df_stock.high.values.reshape(-1,1))
  df_stock['low'] = min_max_scaler.fit_transform(df_stock.low.values.reshape(-1,1))
  return df_stock

#Load Data
def load_data(data, seq_len,train_set_size):
  data_raw = data.as_matrix()
  data = []
  for index in range(0,len(data_raw) - seq_len):
    data.append(data_raw[index:index+seq_len])
  data = np.array(data)
  train_size = int(train_set_size*len(data))
  x_train = data[:train_size,:-1,:]
  y_train = data[:train_size,-1,:]
  x_test = data[train_size:,:-1,:]
  y_test = data[train_size:,-1,:]
  return x_train,y_train,x_test,y_test
  

df = read_data()
visualize_stock()
df = normalize(df,'EQIX')
x_train,y_train,x_test,y_test = load_data(df,20,0.8)
```
<p><img src="/images/data_LSTM.PNG" height="400" width="300"></p>


__Define Model Architecture__
====== 
Now we move on to the main step wherein we construct our model and initialize all its parameters. We are using an LSTM model for training. For simplicity, we will only use a single layer in our model along with some dropout and a Dense layer. 

Recall the gated structure of each cell in the LSTM and the equations corresponding to its pass. As a recap, here is a quick summary- 

Forget Gate-  
<center>$f_{t} = \sigma(W_{f}.[h_{t-1},x_{t}] + b_{f})$</center>
Input Gate-  
<center>$\bar{C_{t}} = tanh(W_{c}.[h_{t-1},x_{t}] + b_{c})$</center>
<center>$i_{t} = \sigma(W_{i}.[h_{t-1},x_{t}] + b_{i})$</center>
Update Cell State-  
<center>$ C_{t} = f_{t}*C_{t-1} + i_{t}*\bar{C_{t}}$</center>
Output-  
<center>$ o_{t} = \sigma(W_{o}[h_{t-1},x_{t}] + b_{0})$</center>  

```python
#Specify Hyperparamters
input_shape = x_train.shape[2]
hidden_size = x_train.shape[2]
num_layer = 2
num_classes = 4
num_epochs = 10000
batch_size = 20
learning_rate = 0.001

x_train = torch.tensor(x_train)
y_train = torch.tensor(y_train)

x_test = torch.tensor(x_test)
y_test = torch.tensor(y_test)

#LSTM Architecture
class LSTM(nn.Module):
  def __init__(self,input_shape,hidden_size,num_layer,num_classes):
    super(LSTM,self).__init__()
    self.input_shape = input_shape
    self.hidden_size = hidden_size
    self.num_layer = num_layer
    self.num_classes = num_classes
    self.lstm = nn.LSTM(input_size=input_shape,hidden_size=hidden_size,num_layers=num_layer,batch_first=True)
    self.dropout = nn.Dropout(0.2)
    self.fc = nn.Linear(hidden_size,num_classes)

  def forward(self,x):
    h0 = torch.zeros(self.num_layer, x.size(0), self.hidden_size).to(device) 
    c0 = torch.zeros(self.num_layer, x.size(0), self.hidden_size).to(device)
    
    # Forward propagate LSTM
    out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)
    
    # Decode the hidden state of the last time step
    out = self.fc(out[:, -1, :])
    return out

model = LSTM(input_shape,hidden_size,num_layer,num_classes).to(device)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```

__Train the Model__
====== 

That's it, we're good to go! We can initialize the model and run it for a fixed number of iterations. Presented as the standard PyTorch training setup, we make a forward pass pass, clear our previous gradients, make a backward pass and make the optimization step.  
```python
epoch_plot = []
loss_plot = []

for epoch in range(num_epochs):
  data = x_train.to(device)
  labels = y_train.to(device)
  epoch_plot.append(epoch)
  
  #Forward pass
  outputs = model(data.float())
  loss = criterion(outputs,labels.float())
  loss_plot.append(loss.item())
  
  #Backward and Optimize
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  #Print Progress
  print ('Epoch [{}/{}], Loss: {:.4f}' 
          .format(epoch+1, num_epochs, loss.item()))
```

__Plot Performance__
====== 

Once training is complete, make sure to plot your learning and see how the model performed. Feel free to play around with hyperparameters and see how the learning varies.  
```python
plt.figure()
plt.plot(epoch_plot,loss_plot)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
```

<p><img src="/images/loss_LSTM.PNG" height="300" width="400"></p>

















