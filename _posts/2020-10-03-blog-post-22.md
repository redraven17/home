---
title: An Introduction to Ising Models as Computational Frameworks
date: 2020-10-03
permalink: /posts/2012/08/blog-post-22/
author_profile: false
tags:
  - Reinforcement Learning
  - Deep Learning
---


For long, Deep Learning has been motivated by the design and implementation of physics-based methods such as Boltzmann Machines and energy-based distributions for probabilistic estimations. This blog post takes the idea of physical implementations in deep learning a step further and introduces Ising models. Ising models have been the fundamental aspect of computations and approximations in the machine learning community. In fact, the famous neural network being used for majority of machine learning applications is itself a specific case of an Ising model carrying out probabilistic inference using a boltzmann distribution (also known as the softmax activation in the machine learning community). We will look at the need for adopting Ising models in the machine learning community and their capabilities from a computational perspective. Additionally, the later sections of the post will dive deeper into the working and operation of the model along with its various components. 

__Why the Ising Model?__
======  
The early 1920s saw physicists working on lattice structures which were intrinsically difficult to solve. These structures consisted of inherent computational properties and their dynamic behavior in different phases opened multiple questions about their physical and chemical characteristics. Naturally, solving these lattices became of interest to physicists who originally worked on lower dimensional models. Ernst Ising (below), a german physicist, solved the one-dimensional Ising Model in 1925 which was given to him as a problem by his advisor Wilhelm Lenz (below). Ising solved the model and indicated that no phase transitions occured between particles which would later be corrected for higher dimensions. Although Ising solved the one-dimensional model, the two dimensional square-lattice model is much harder to solve and was later looked at analytically by Lars Onsager in 1944. The two-dimensional can now be solved using transfer matrix method which is one of the many techniques used for solved lattices alongside quantum theory.  

<p align="center"><img src="/images/ising.png" height="200" width="200" />   <img src="/images/lenz.jfif" height="200" width="200" /></p>  
<p align="center"><em>Ernst Ising (1924), Wilhelm Lenz (1920)</em></p>  

One of the reasons why Ising model is famous for its lattice structure is that it lays out the interaction between its particles in an energy-based manner. Particles acquire spin values which allows them to interact with adjacent particles in the model, hence impacting the overall configuration of teh system. Eventually, particle-particle (or rather spin-spin) interactions end up minimiznig the energy of the system which results in a thermal equilibrium. Additionally, the model temperature can be annealed in simlutaion to excite the particles in a more drastic manner and assess their behavior which would eventually result in thermal equilibrium. Thermal equilibrium is a desirable from a computational perspective as it guarantees convergence in an algorithm. For instance, consider an algorithm which was constructed to solve the Ising model and does not have any knowledge of its particle positions. The algorithm will begin by tuning the temperature and particle spins in order to make the model converge. Once the model is converged, the problem is considered to be solved, indicating that the problem has atleast one solution. The equilibrium property of Ising model assures modern-day programmers that the problem can be solved and efficient solutions to approximations may exist.  

However, existence of a solution does not serve as a valid motviation for the usage of Ising model since there are multiple frameworks which guarantee the existence of many solutions in algorithms. The primary reason for the usage of Ising model is its structure. Ising model constitutes of particles arranged in a random configuration which rearrange themselves to yield optimal energy. Thi can be directly translated to modern-day neural networks which consist of particles (neurons) that adjust their weights (shift their positions in the lattice) to yield an optimal solution (energy). The deep structural connection between Ising lattices and computational graphs served as one of the major motivations for their adoption into the machine learning community back in the early 1970s. A number of different inference frameworks such as the Boltzmann Machine (BM), Restricted Boltzmann Machine (RBM) and Hopfield Network (HN) utilized energy-based objectives which were motivated by Ising computations. Additionally, the HN served as one of foundational examples of memory-based learning leveraging the interaction between neurons which is analogous to determining optimal spin-spin configurations in the lattice.  

<p align="center"><img src="/images/struct.jfif" height="200" width="200" />   <img src="/images/rbm.png" height="200" width="300" />   <img src="/images/hn.png" height="200" width="200" /></p>  
<p align="center"><em>Structural similarity between 2D Ising Model (left), RBM (center) and HN (right) highlighting the widespread adoption of the lattice framework</em></p>  

__But what exactly is an Ising Model?__
======  
Although there have been many definitions of the Ising model, we will look at the two most famous ones. We willf first define the ising model from a physical perspective as Ising did and then gain intuition to understand its definition from a computational perspective.  

The Ising Model is an energy-based model which consists of physical particles in a lattice structure. Particles acquire spin states in the lattice which affect the states of neighbouring particles as well as the overall configuration of the system. Consider an Ising model consisting of $N$ particles with $N \geq 1$ and $N \neq \infty$. The set of all particles can then be represented as $\{1,2,...,N\}$. Mathematically, let $\sigma_{i}$ be the spin state of particle $i$ in the system, then $\sigma_{i}$ may take a value from the set $\mathcal{S}$ which comprises of all the possible spin states. Most Ising models are bivariate which consist of only two spin states $\mathcal{S} = \{-1,+1\}$. In physics, the two spin states correspond to clockwise and counter-clockwise spins of particles of a ferromagnetic material.

<p align="center"><img src="/images/model.gif" height="100" width="300" /></p>  
<p align="center"><em>Bivariate spin configurations in a 1D Iising model</em></p>  

However, modern Ising models are often presented to contain more than two spin states. Particles in an Ising model interact with each other by means of their spin states. Consider two particles $i$ and $j$ with spin states $\sigma_{i}$ and $\sigma_{j}$. The interaction between the two particles $<\sigma_{i},\sigma_{j}> = \mu_{ij}$ can be mathematically expressed as $\mu_{ij} = \sigma_{i}.\sigma_{j}$. These interactions are also called spin-spin interactions since they take place between spin states of the two particles.

Computationally, the spin-spin interactions between any two particles can be regarded as a connection between two nodes of a computation graph. Consider two nodes A and B. These nodes interact with each other by means of a weight which determines the extent of information to be passed from node A to node B. More intuitively, the weight can be thought of as a controlling mechanism for governing the flow of input. In the case of Ising model, the weight can be thought of as an interaction between two particles which governs the overall change in the system. The value of the weight plays an essential role for the system to achieve thermal equilibrium. Thus, much of the properties of Ising models are leveraged using their particles and spin values from a computational perspective.  

__Math, math, math ...__
======  
We will now dive deeper into the mathematical formulation of Ising models and try to gain intuition from the energy-based objective of the Ising framework. So buckle up!

Considering an external magnetic field $h_{j}$ on particle $j$, we can express the total energy $E$ of the Ising model using the Hamiltonian function as followed-  

$    E(\sigma_{i},\sigma_{j}) = - \sum_{<i,j>} J_{ij}\sigma_{i}\sigma_{j} - \sum_{j}h_{j}\sigma_{j}, \qquad \forall i,j \in \{1,2,...,N\}$  
$    = \sum_{<i,j>} J_{ij}\mu_{ij} - \sum_{j}h_{j}\sigma_{j} \qquad \forall i,j \in \{1,2,...,N\}$  

Here, $<i,j>$ represent the indices of all particles in the Ising model, $J_{ij}$ indicates an interaction parameter which quantifies the probability of interaction between $i$ and $j$ particles. Note that the total energy $E(\sigma_{i},\sigma_{j})$ is a function of only the spin states of particles indicating that only the internal composition of the lattice is responsible for minimizing the energy of the system. Moreover, the partition function $Z_{\beta}$ corresponding to the energy function is given below. Here $\beta = \frac{1}{T}$ represents the inverse of temperature $T$.  

$Z_{\beta} = \sum_{\sigma_{i},\sigma_{j}}\exp{(-\beta E(\sigma_{i},\sigma_{j}))} \qquad \forall i,j \in \{1,2,...,N\}$  

Given the energy $E(\sigma_{i},\sigma_{j})$ and partition function $Z_{\beta}$ corresponding to the states of the system, we can express the configuration probability $P_{\beta}(\sigma_{i},\sigma_{j})$ of the model using the Boltzmann distribution with $\beta \geq 0$. $P_{\beta}(\sigma_{i},\sigma_{j})$ for a given configuration is presented below as the ratio between the configuration $\exp{(- \beta E(\sigma_{i},\sigma_{j}))}$ and the partition function $Z_{\beta}$ denoting the sum over all possible configurations. The negative sign in the exponent denotes assigns a higher probability to low energy states. Similarly, a high $\beta$ value accentuates the probabilities of the low energy states. Upon carrying out simulated annealing of $T$ closer to the low energy states, the system tends to reach thermal equilibrium.  

$P_{\beta}(\sigma_{i},\sigma_{j}) = \frac{\exp{(-\beta E(\sigma_{i},\sigma_{j}))}}{Z_{\beta}} \qquad \forall i,j \in \{1,2,...,N\}$  

The partition function aids in the assessment of necessary thermodynamical and computational quantities such as internal energy $U$ which is defined as the negative derivative of the partition function $Z_{\beta}$ \cite{stats}. U is mathematically expressed below.   

$U = - \frac{\partial Z}{\partial \beta} = - \frac{1}{Z}\sum_{\sigma_{i}\sigma_{j}}\exp{(-\beta E(\sigma_{i},\sigma_{j}))} \qquad \forall i,j \in \{1,2,...,N\}$  

Similarly, the free energy per particle $F$ can be obtained as the average of log partition $\log{Z_{\beta}}$ in the limit of the number of particles in the model. This is mathematically expressed below.  

$F = F(\beta,E,N) = \lim{N \to \infty} \frac{1}{N} \log{Z_{\beta}(E,N)}$  

The limit $N \to \infty$ in the expression is called the thermodynamic limit. In practical scenarios, computation of the free energy $F$ is non-trivial as there is no guarantee of the existence of thermodynamic limit $N \to \infty$ as the lattice system can grow at different rates and in different directions. Such constraints on the growth and size of the system hinder a closed form expression for $F$ and hence pose restrictions on the solution of the lattice. Moreover, the structure and complexity of the model increases with the number dimensions which further makes simulations intractable and inaccurate by making use of approximations. As a result of this, most modern computational frameworks  make use of the energy and partition functions in an indirect manner by making use of structures which can be realized and yield probabilistically approximate estimates.  


__Simulating a 2D Ising Model__  
======  
This section of the blog is borrowed from [this amazing blog](https://rajeshrinet.github.io/blog/2014/ising-model/) which simulates a 2D Ising model in Python. We will follow a similar setup wherein we will first simulate the model and observe its configurations at different time intervals to observe the energy function optimize in simulation. The model starts with a random configuration and achieves its equilibrium state within 1000 timesteps.  

```python
%matplotlib inline
# Simulating the Ising model
from __future__ import division
import numpy as np
from numpy.random import rand
import matplotlib.pyplot as plt

class Ising():
    ''' Simulating the Ising model '''    
    ## monte carlo moves
    def mcmove(self, config, N, beta):
        ''' This is to execute the monte carlo moves using 
        Metropolis algorithm such that detailed
        balance condition is satisified'''
        for i in range(N):
            for j in range(N):            
                    a = np.random.randint(0, N)
                    b = np.random.randint(0, N)
                    s =  config[a, b]
                    nb = config[(a+1)%N,b] + config[a,(b+1)%N] + config[(a-1)%N,b] + config[a,(b-1)%N]
                    cost = 2*s*nb
                    if cost < 0:	
                        s *= -1
                    elif rand() < np.exp(-cost*beta):
                        s *= -1
                    config[a, b] = s
        return config
    
    def simulate(self):   
        ''' This module simulates the Ising model'''
        N, temp     = 64, .4        # Initialse the lattice
        config = 2*np.random.randint(2, size=(N,N))-1
        f = plt.figure(figsize=(15, 15), dpi=80);    
        self.configPlot(f, config, 0, N, 1);
        
        msrmnt = 1001
        for i in range(msrmnt):
            self.mcmove(config, N, 1.0/temp)
            if i == 1:       self.configPlot(f, config, i, N, 2);
            if i == 4:       self.configPlot(f, config, i, N, 3);
            if i == 32:      self.configPlot(f, config, i, N, 4);
            if i == 100:     self.configPlot(f, config, i, N, 5);
            if i == 1000:    self.configPlot(f, config, i, N, 6);
                 
                    
    def configPlot(self, f, config, i, N, n_):
        ''' This modules plts the configuration once passed to it along with time etc '''
        X, Y = np.meshgrid(range(N), range(N))
        sp =  f.add_subplot(3, 3, n_ )  
        plt.setp(sp.get_yticklabels(), visible=False)
        plt.setp(sp.get_xticklabels(), visible=False)      
        plt.pcolormesh(X, Y, config, cmap=plt.cm.RdBu);
        plt.title('Time=%d'%i); plt.axis('tight')    
    plt.show()
```

```python
rm = Ising()
rm.simulate()
```
Upon simulation we observe the following configurations-  
<p align="center"><img src="/images/simulation.PNG" height="500" width="700" /></p>  

As stated before, the model converges to an equilibrium state by making use of spin-spin interactions and the lattice is declared to be stable within 1000 timesteps.  

__Generalizing lattices in $n$-dimensions__
======  

<p align="center"><img src="/images/3d.gif" height="400" width="400" /></p>  
<p align="center"><em>3D Ising model rearranging its particle spins in simulation</em></p>  

So far we have looked at  1D and 2D Ising models which have well-efined lattice structures. In the case of $n$-dimensional Ising models, things tend to become complicated. The Ising model in $n$-dimensions consists of multiple interactions per particle which are difficult to solve for givent eh visual constraints. Additionally, the presence of an external magnetic field in the case of $n$-dimensions add a new component of energy which needs to be solved. While the generalized $n$-dimensional Ising model remains an open problem from a computational perspective, physical properties of lower dimensional Ising models readily translate to higher dimensions. For instance, the model is guaranteed to achieve a minimum energy configuration at thermal equilibrium and leverages the spin-spin interactions to do so. Additionally, the bivariate spins can be generalized to multiple spin moments (as in the case of Heisenberg model) which may yield diverse ccomputational techniques in leanring theory. 



